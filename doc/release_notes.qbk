[/============================================================================
  Boost.AFIO
  
  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:release_notes Release Notes]

[/=================]
[heading Anticipated forthcoming features in future versions]
[/=================]

* It is hereby noted that the v1.2 engine will be the last to support Microsoft
Visual Studio 2010. v1.3 engines will [*require] Microsoft Visual Studio 2012
or better.

* The v1.3 engine will gain a new concurrent unordered_map implementation giving
still further significant performance gains. This will remove all locking completely
from AFIO code.

* The v1.4 engine will have an async fast batch hash engine which provides
transparent hashing of all async reads and writes.
  * New completion handler metaprogramming letting you bind add hash to each
op easily.
    * Automatic ASIO callback prototype spec support.

[subheading In some later version:]

* Multiple dispatcher support
  * Integration of direct proposed Boost.Fiber library support as a secondary
thread_source to std_thread_source.

* async_io_dispatcher_base::read_partial() to read as much of a single buffer as
possible, rather than only complete buffers.

* Portable fast file locking which works across network shares, but can still
utilise shared memory when possible.

* Fast, scalable portable directory contents change monitoring. It should be able
to monitor a 1M entry directory experiencing 1% entry changes per second without
using a shocking amount of RAM.


[/=================]
[heading Boost 1.56 AFIO v1.22 trunk]
[/=================]

Finished getting a ThreadSanitizer (tsan) + UndefinedBehaviorSanitizer (ubsan) pass
running per-commit on Travis CI (>= v1.2 was tsan clean, I just hadn't bothered getting
a CI to verify it to be so per commit).

Fixed bug where --lto wasn't turning on the optimiser for LTO output. Sorry.

Added a benchmark testing for latency under concurrency loads.

Added a new FAQ entry on AFIO execution latencies.

Reorganised source code structure to fit modular Boost. AFIO is now a Boost v1.56
module just like any other. Obviously this will break source code compatibility
with all preceding Boosts.

[/=================]
[heading Boost 1.55 AFIO v1.21 23rd Mar 2014]
[/=================]

Fixed a bug in the custom unit testing framework which was throwing away any exceptions
being thrown by the tests (thanks to Paul Kirth for finding this and reporting it).
Fixing this bug revealed that enumerate() with a glob on Windows has never worked
properly and the exception thrown by MSVC's checked iterators was hiding the problem,
so fixed that bug too.

Added async_io_dispatcher_base::post_op_filter() and async_io_dispatcher_base::post_readwrite_filter(),
including documentation examples and integrating filters into the unit testing.
post_readwrite_filter() ought to be particularly useful to those seeking deep ASIO
integration. Thanks to Bjorn Reese for the long discussions leading up to this
choice of improved ASIO support.

During updating the benchmarks below now I have regained access to my developer
workstation, discovered a severe performance regression in the v1.2 engine of
around 27% over the v1.1 engine. Steps taken:

1. The shared state in every async_io_op was a shared_ptr, now it is the
underlying shared_future. Eliminated copies of shared_ptr, now we always
use the shared_future in enqueued_task directly. This reduced regression to 18%.

2. Removed more code from inside the TSX locks. This reduced regression to 16%.

3. Removed the second TSX lock from complete_async_op(). This eliminated the
regression and actually added 2% to the v1.1 engine.

4. Removed the second TSX lock from chain_async_op(). This added a further 10%
over the v1.1 engine, so we are now 12% faster which is about right given
the v1.2 engine removed 15% of code.

Added nested TSX transaction support.

The CI shows that clang 3.1 now produces segfaulting binaries with this release,
so rather than debug clang, I simply dropped clang 3.1 support. AFIO now requires
clang 3.2 or better.

[/=================]
[heading Boost 1.55 AFIO v1.20 5th Feb 2014]
[/=================]

This is a major refactor of AFIO's core op dispatch engine to trim it down by
about 15%. Key breaking differences from the v1.1 series of AFIO are as follows:

* Replaced all use of packaged_task with enqueued_task, a custom implementation
which makes possible many performance improvements throughout the engine.
* thread_source::enqueue() now can take a preprepared enqueued_task.
* thread_source::enqueue() now always returns a shared_future instead of a future.
This has had knock on effects throughout AFIO, so many futures are now shared_future.
* Completion handler spec has changed from:
 
   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, shared_ptr<async_io_handle> h, exception_ptr *e)
 
  to:

   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, async_io_op preceding)
 
  This substantially improves performance, simplifies the implementation, and lets
  completion handlers more readily retrieve the error state of preceding operations
  and react appropriately.
* All restrictions on immediate completions have been removed. You can now do anything
in an immediate completion that you can do in a normal completion.
* async_io_op::h now always refers to a correct future i.e. the future is no longer
lazily allocated.
* Now that op futures are always correct, when_all(ops) has been drastically simplified
to an implementation which literally assembles the futures into a list and passes
them to boost::wait_for_all().
* Added when_any(ops).

[/=================]
[heading Boost 1.55 AFIO v1.11]
[/=================]

Added --fast-build to test Jamfile to preserve my sanity attempting to work with
AFIO on an Intel Atom 220 netbook.

Fixed failure to auto-const an async_data_op_req<boost::asio::mutable_buffer>
when used for writing. Thanks to Bjorn Reese for reporting this.

Replaced use of std::runtime_error with std::invalid_argument where that makes
sense. Thanks to Bjorn Reese for reporting this.

Replaced throwing of std::ios_base::failure with std::system_error. Thanks to
Bjorn Reese for suggesting and submitting a patch for this.

async_io_dispatcher_base::enumerate() did not take a metadata_flags, and it
was supposed to. Thanks to Bjorn Reese for reporting this.

Added a unit compilation test to ensure that implicit construction from a
single arg to the op convenience classes works as intended.

Significantly optimised build system and added in precompiled headers support.
Combined with --fast-build this provides an 8x build time improvement.

boost::afio::stat_t::st_type() is now a boost::filesystem::file_type instead
of replicating the POSIX file type codes. Thanks to Bjorn Reese for suggesting
this.

boost::afio::stat_t::st_mode() is now st_perms(). Also disabled unused fields in
stat_t on Windows. Thanks to Bjorn Reese for suggesting this.

[/=================]
[heading Boost 1.55 AFIO v1.1 1st Nov 2013]
[/=================]

Immediate completions no longer hold the opslock, which meant the opslock could be
changed from a recursive mutex to a spinlock. The new, more parallelised, behaviour
illuminated a number of new race conditions in when_all() which have been fixed.

Completely gutted dispatch engine and replaced with a new, almost entirely wait
free implementation based on throwing atomics at the problem. If it weren't for the spin lock around the
central ops hash table, AFIO would now be an entirely wait free design.

In order to do something about that spin lock, replaced all locking in AFIO (apart
from the directory file handle cache) with memory transactions instead. This
does CPUID at runtime and will use Intel's TSX-NI memory transaction implementation
if available, if not it falls back to a spin lock based emulation. On memory
transaction capable CPUs, AFIO is now almost entirely wait free, apart from when
it has to fetch memory from the kernel.

Made AFIO usable as headers only.

[/=================]
[heading Boost 1.55 AFIO v1.0 27th Sep 2013]
[/=================]

First release for end of Google Summer of Code 2013.

[endsect]




[section:FAQ Frequently Asked Questions]

[section:closure_performance What is the maximum throughput of AFIO's closure execution engine aka
how many IOPS can I push with AFIO?]

For v1.21 of the engine, maximum ops [*dispatch] throughput is approximately as follows, where the values for `call()` might be for
use as a closure engine whereas the values for `completion()` might be for max filing system IOPS[footnote The phrase
["might be] is important: a null closure benchmark will always have dispatch rate problems i.e. the closures being executed
take less time to execute than the time to dispatch them, so these figures are best read as maximum dispatch rate, not
maximum IOPS.]:

[table:throughput Maximum null closure dispatch rate on a 3.5Ghz Intel Core i7 3770K for AFIO v1.21[footnote Not using Intel TSX.
Benchmarks compiled using `address-model=64 link=static --lto release -j 4`.]
[[Operating system][`call()` unchained][`call()` chained][`completion()` unchained][`completion()` chained][Raw ASIO]]
[[Microsoft Windows 8 x64 with Visual Studio 2013][[role alignright 1133730]][[role alignright 605569]][[role alignright 1629110]][[role alignright 647738]][[role alignright 2376440]]]
[[['Relative to ASIO]][[role alignright 48%]][[role alignright 25%]][[role alignright 69%]][[role alignright 27%]][[role alignright 100%]]]
[[['Relative to AFIO v1.1]][[role alignright +24%]][[role alignright +2%]][[role alignright +5%]][[role alignright -11%]][[role alignright 100%]]]
[[][][][][][]]

[[Ubuntu 12.04 LTS Linux x64 with GCC 4.8][[role alignright 1291250]][[role alignright 1022970]][[role alignright 1479540]][[role alignright 1200450]][[role alignright 1611040[footnote Unfortunately the glibc mutex used by ASIO does not scale well on modern CPUs due to having too small a spin count, so this value is for four threads instead of eight (it was the best I could get through trial and error).]]]]
[[['Relative to ASIO]][[role alignright 80%]][[role alignright 64%]][[role alignright 92%]][[role alignright 75%]][[role alignright 100%]]]
[[['Relative to AFIO v1.1]][[role alignright +18%]][[role alignright +29%]][[role alignright +3%]][[role alignright +24%]][[role alignright 100%]]]
[[][][][][][]]

[[Microsoft Windows 8 x64 with Visual Studio 2010][[role alignright 930424]][[role alignright 709952]][[role alignright 1495740]][[role alignright 697333]][[role alignright 2393570]]]
[[['Relative to ASIO]][[role alignright 39%]][[role alignright 30%]][[role alignright 62%]][[role alignright 29%]][[role alignright 100%]]]
[[['Relative to AFIO v1.1]][[role alignright +43%]][[role alignright +51%]][[role alignright +46%]][[role alignright +14%]][[role alignright 100%]]]
]

Note how Visual Studio with its incomplete C++ 11 support does not fare well against a fully C++ 11 compliant compiler. In fact,
with the v1.21 engine old VS2010 riddled with compatibility shims does surprisingly well, the new much simpler implementation really
favours VS2010 to the extent it's actually faster than VS2013 for chained ops.

We hope that ~600k min IOPS surely ought to be enough to max out any SATA III SSD __dash__ it should even max out any mid range
PCIe based SSD too, and if paired with a CPU with more cores, AFIO's almost wait free design ought to scale out fairly
well too such that it may be able to max out even top end PCIe SSDs, some of which can push 10m IOPS now. If that isn't
fast enough for you, try AFIO on an Intel CPU with memory transaction support (a runtime check will automatically
replace locks with TSX memory transactions), or try support for `__transaction_relaxed` compilers if `BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER` is defined.

[endsect] [/closure_performance]

[section:closure_latency What is the latency of AFIO's closure execution engine aka
how quickly can I push a single op with AFIO?]

For v1.21 of the engine on a quad core hyperthreading 3.5Ghz Intel Core i7 3770K, I found
the following latency graph which is in CPU cycles:

[role aligncenter [$afio_latencies.svg]]

The solid lines represent average dispatch to execution and execution finish to completion
notification latencies and are cumulative on the left scale, whilst the dotted lines represent
minimum latencies and are cumulative on the right scale. As one can see, one can expect a
minimum of around 6,000 CPU cycles between dispatch and the operation beginning, and a minimum
of around 4,000 cycles between the operation completing and its op reference becoming signalled,
thus making total minimum latency per op around 10,000 CPU cycles, or around 2.5 microseconds.

Average latency is rather worse of course, but if you exceed CPU cores by just enough to stop
them clocking down you can expect a 300,000 CPU cycle total latency with a 95% confidence
interval of plus/minus 25,000 CPU cycles, which is 75 microseconds plus/minus 6.25 microseconds.
Obviously this makes AFIO unsuited for low latency work such as high frequency trading, but once
you get below 0.1 milliseconds it becomes very hard to disentangle SSD access times from the
latency of the filing and storage systems between you and the SSD anyway, so even if AFIO might
appear to on average double the 0.08 millisecond access time a SSD might have, you really must
bear in mind that a thread sleep and wake costs about 215,000 CPU cycles, and the obvious thing
which is happening in the benchmark is that AFIO is on average sleeping and waking 1.33 times
per operation dispatched and completed. Your use case will of course vary, and if you hit
the spin locks right I've personally seen 9,000 CPU cycle sustained average latencies.

[endsect] [/closure_latency]

[section:stuck_ops I'm seeing ["WARNING: `~async_file_dispatcher_base()` detects stuck `async_io_op` in total of X ops] during
process close. What does this mean?]

This means that you scheduled ops with a dispatcher which did not complete in a timely fashion before you tried to destroy
that dispatcher. This generally indicates a bug in your code, which can include:

# An op is still running in some thread pool and you didn't use __afio_when_all__ to wait for it to complete before trying
to destroy the dispatcher.
# An op's precondition never completed and therefore the op was never started.

Tracking down the cause of the latter in particular is very similar to tracking down race conditions i.e. it's hard, and we
as the authors of __boost_afio__ know just how hard! If you are on POSIX, recompiling AFIO with the macro
`BOOST_AFIO_OP_STACKBACKTRACEDEPTH` set to a reasonable depth like 8 will have AFIO take a stack backtrace for every op
allocated which it then will print during the stuck ops warnings. This can be helpful to identify which ops exactly are
stuck, and then you can figure out which preconditions of theirs are the cause.

Note that `BOOST_AFIO_OP_STACKBACKTRACEDEPTH` has two implementations, one based on glibc `backtrace()` and the other based
on libunwind. The latter is more portable, but requires explicit linking with libunwind, so we have defaulted to the former.

[endsect] [/stuck_ops]

[section:vector_use Why did you use `std::vector<>` as the ops batch container instead of a generic iterator range?]

# `std::vector<async_io_op>` is the closest thing to a variable length array in C++[footnote Ok, there is also
the oft-forgotten `std::valarray<>` too, but its use as a generic container isn't recommended.], at least until C++ 14 where we
will gain `std::dynarray<>` and dynamic array sizing.
# `std::vector<async_io_op>` is well understood, particularly its performance during by-value copies and during
`push_back()` which is by far the most common operation you do when preparing a batch.
# `std::vector<async_io_op>` is very amenable to splitting the batch across threads (not that AFIO currently does this).
# `std::vector<async_io_op>` is easily transportable through an ABI, whereas arbitrary container iterators would need type
erasing (i.e. slow). As AFIO was developed initially as not header-only, this made a lot of sense initially.

We are not opposed to the use of generic iterator ranges in an AFIO v2 if there is user demand for such a thing.

[endsect] [/vector_use]

[section:foreign_fd How do I configure my own file descriptor or HANDLE in AFIO?]

Sometimes you receive a file descriptor or HANDLE from a third party piece of code and you need to insert it as-in
into AFIO for use. The solution to this is very simple:

# Subclass __afio_handle__ with a custom implementation for your file descriptor type. In particular, you probably
will have custom close semantics (e.g. don't close, or invoke third party code to implement close).
# Instantiate your custom handle implementation, and pass it into `async_file_io_dispatcher_base::adopt()`. This
will immediately convert your custom handle type into an `async_io_op` suitable for supplying to `read()`, `write()`
etc.
# That's it, there is no more to it.

[endsect] [/foreign_fd]

[section:slow_compile Using AFIO really slows down my compile times. Can't you do something about that?]

You'll find a huge amount depends on your compiler. Here are some build time benchmarks for my developer workstation:

[table:build_time Single thread build times for AFIO v1.2 for various compilers and options on a 3.5Ghz Intel Core i7 3770K
[[Build flags][Microsoft Windows 8 x64 with Visual Studio 2013][Ubuntu 12.04 LTS Linux x64 with GCC 4.8 and gold linker][Ubuntu 12.04 LTS Linux x64 with clang 3.4 and gold linker]]
[  [`--link-test --fast-build debug`][01m43s][03m31s[footnote ASIO has a link error without `link=static`]][fails]]
[               [`--link-test debug`][02m40s][06m15s][04m24s]]
[         [`--link-test --lto debug`][[role red 09m36s]][06m23s][04m32s]]
[       [`--link-test pch=off debug`][06m12s][08m01s][04m26s]]
[[`--link-test --fast-build release`][02m01s][03m15s[footnote ASIO has a link error without `link=static`]][fails]]
[             [`--link-test release`][03m09s][06m28s][06m06s]]
[       [`--link-test --lto release`][10m24s][07m08s][06m14s]]
]

The benefit of `--fast-build` grows exponentially the less powerful the CPU: on my Intel Atom 220
netbook `--fast-build` gets total build times under five minutes which is impressive compared to
the twenty or so minutes without that flag.

`--fast-build` works by defining `BOOST_AFIO_HEADERS_ONLY` to 0 for each compiland, and
linking to the AFIO shared library instead. It also does the same for ASIO by defining `BOOST_ASIO_SEPARATE_COMPILATION`
and `BOOST_ASIO_DYN_LINK` for each compiland, and links to the ASIO
shared library instead (which is built via `boost/asio/impl/src.hpp`, see ASIO's docs).

[endsect] [/slow_compile]

[section:fatal_error_read Why do I get a fatal application exit with `FATAL EXCEPTION: Failed to read all buffers` when I read a file?]

This is actually a safety checkpoint for your code: in complex, multi-process concurrent reading and writing of
the same file, it is extremely difficult to coordinate changing file lengths with i/o in a way which
doesn't introduce race conditions OR unacceptably low performance. AFIO therefore doesn't even
try[footnote AFIO [*will] try to provide a synchronised, accurate file extent after fast portable
file locking support has been added, but until then no.]
and simply requires you the programmer to ALWAYS do i/o, whether reading or writing, within the extent
of a file. In other words, if you're going to read 100 bytes from offset 100 in a file, that file
better be at least 200 bytes long or it's going to fail with a fatal application exit.

This will probably seem harsh to anyone using AFIO for the first time, because the following
naive code will fatal exit the application if foo.txt is not 1024 bytes or longer:

[readallof_example_bad]

With synchronous i/o a read of 1024 bytes will read ['up to] 1024 bytes, returning the amount
actually read via some mechanism. With AFIO, either you
read [*all] 1024 bytes or you read nothing, in which case a normal exception is thrown with
whatever error the operating system returned. If a ['partial] read happens, then AFIO fatal
exits the application with the above message as it is probably a logic error in your code.

You may now wonder how to easily not exceed file extents during i/o: for writing, see
__afio_truncate__ to ensure a file's size before writing. For reading, the following code
is suggested:

[readallof_example_single]

If you're going to read many files from the same directory, it is far faster to open a
handle to the containing directory and using enumerate to fetch the metadata asynchronously
instead of using `direntry()` which is synchronous:

[readallof_example_many]

[endsect] [/fatal_error_read]

[section:async_metadata `async_io_handle::direntry()` and `async_io_handle::lstat()` are
both synchronous functions which block. How then can I get metadata about files and
directories asynchronously?]

This is easy, if not terribly obvious: call __afio_enumerate__ on the containing
directory of the file you want metadata for with a shell glob exactly matching the
file name and with the minimum metadata you are looking for. AFIO will then
asynchronously fetch that metadata for you, returning it in the future `directory_entry`
returned by `enumerate()`.

[endsect] [/async_metadata]

[endsect] [/FAQ]
