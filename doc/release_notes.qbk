[/============================================================================
  Boost.AFIO
  
  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:release_notes Release Notes]

[/=================]
[heading Anticipated forthcoming features in future versions]
[/=================]

* The v1.4 engine will be rewritten yet again to use a new custom future
implementation whereby async_io_op shall become afio::future<T>. This should
let the API no longer return two sets of futures when returning results
(async_io_op therefore matches afio::future<void>), plus make best use of the
proposed concurrent_unordered_map by finally actually processing batches of
operations as a batch, instead of one at a time.

* The v1.4 engine afio::future<T> ought to transparently support Boost.Fiber,
this should let you program against AFIO using awaitable resumable functions
which is much cleaner. A big hope is that the stochastic variance in op
processing latency should be heavily reduced (currently a 300k cycle average,
min 10k cycles).

* The v1.4 engine will finally make use of the API support for alternative async_io_dispatcher
implementations, adding at least one for temp file support with special temp
file semantics, and maybe others with transparent hashing and bit flip healing
(see below). The temp file support will allow anonymous and named temp files
which can be device volume matched to some path, thus allowing file move to be
done without copying. Anonymous temp files can use Linux specific facilities
for those.

* Finally, making good use of the new coroutine support the v1.4 engine should
have an async fast batch hash engine which provides transparent hashing of all
async reads and writes. Chances are good one can also choose to generate SECDED
Error Correcting Codes as I have a test implementation here which can process
~500Mb/sec for an enormous (32784, 32768) Hamming code which lets you heal one
bit flip per 4Kb page, ideal for working with non-ECC RAM systems where bit flips
between storage and RAM are frighteningly common (1e-12 flips/bit/hour or so
potentially, that's 3.3 bits flipped in a 16Gb RAM system per 24 hours). See
Schroeder, Pinheiro and Weber (2009) 'DRAM Errors in the Wild: A Large-Scale
Field Study'.


[h5 In some later version, in order:]

* Individual file change monitoring. This would be very useful for implementing
distributed mutual exclusion algorithms to avoid spinning on file updates.

* Related to the preceding item, formal async lock file support with deadline
timeouts.

* Portable fast file byte range advisory locking which works across network shares,
but can still utilise shared memory when possible.

* Extended attributes support. TripleGit could use this to avoid a second file
handle open of metadata per graph object read. Unsure if NTFS is any faster
opening EA though, need to test.

* async_io_dispatcher_base::read_partial() to read as much of a single buffer as
possible, rather than only complete buffers.

* Fast, scalable portable directory contents change monitoring. It should be able
to monitor a 1M entry directory experiencing 1% entry changes per second without
using a shocking amount of RAM.

* ACL support

* Asynchronous file handle closing in ~async_io_handle() (currently if not
explicitly closed, the async_io_handle destructor must synchronously close)


[/=================]
[heading Boost 1.57 AFIO v1.3x unstable branch]
[/=================]

AFIO is now a Boost.BindLib based library. This has resulted in an enormous change set
which is only barely summarised here:

* AFIO is now capable of:
  * Being used standalone, or as a Boost module, or if you have inline namespace support in your
  compiler then both simultaneously in the same translation unit or in the same binary, including
  any combination of the following library dependency configurations (config macro and its default
  is shown):
  * Using Boost.Atomic, Boost.Chrono and Boost.Thread OR the C++ 11 STL (BOOST_AFIO_USE_BOOST_THREAD=0).
  * Using Boost.Filesystem OR the C++ 1z Filesystem TS (BOOST_AFIO_USE_BOOST_FILESYSTEM=1, except on
  VS2015 which provides Filesystem).
  * Using Boost.ASIO OR standalone ASIO (ASIO_STANDALONE=0)
  
  That makes eight different potential configurations, and all eight can coexist in the same translation unit,
  though you will find compilation time becomes enormous.

* AFIO's unit test suite no longer requires Boost.Test, and can now alternatively use CATCH C++
(BOOST_AFIO_USE_BOOST_UNIT_TEST=0) via BindLib.

* Dropped support for these compiler versions due to insufficient C++ 11 support:
  * Anything before GCC 4.7, as these lack template alias support.
  * Anything before VS2013, as these lack template alias support. I took the opportunity to clean out
  the VS2010 special code paths and all the variadic template emulation.

* symlink() now can create file symbolic links on Windows (it'll probably error out due to lack of
privileges held by the user, but it can now at least try).

* Added the TemporaryFile and DeleteOnClose file_flags. These improve lock file performance on Windows
by about 60%.

* Added sparse file support. AFIO now always creates sparse file where possible, and converts any
files it opens for writing into a sparse file where possible. You can disable that behaviour using
the NoSparse flag.

* stat_t now contains member flags indicating if a file entry is sparse and/or compressed.

* Added a new api zero() which can very efficiently zero ranges in a file by deallocating them on
physical storage (["hole punching]).

* Added a new api extents() which lets you query which ranges in a file contain valid data.

* Added a new api statfs() which returns a statfs_t which lets you query the volume on which a file
or directory lives. Again, thanks to using the NT kernel API directly the structure on Windows is
almost as complete as on POSIX.

* Improved unit testing, especially unit testing of error handling, and indeed found that on Windows
an invalid handle object generated by a failed open or an explicit close caused the next dependant
operation to segfault. Also on Windows any errors generated during read() and write() were being lost
and the dispatcher hanged instead of being reported (oops!).

* Added an extra section to the tutorial on how to implement an atomic log file using the new
features in AFIO.

* Completely rewrote the async_data_op_req metaprogramming which assembles ASIO scatter gather buffers
from types and containers supplied to read() and write(). The old system which had to work on VS2010
was basically a set of repetitive hardcoded template specialisation overloads for void *, T *,
std::array, std::vector and std::basic_string. The new much more sophisticated metaprogramming (with
an unfortunate corresponding increase in compilation times) now understands any STL like container
(detected using SFINAE std::begin() and std::end()), including nested STL like containers, and will
correctly generate ASIO scatter gather buffers from say something like a std::list<std::list<std::string>>
but also correctly coalesce buffers for trivial types such as std::vector<std::array<trivial type, N>> where it spots that
only one ASIO scatter gather buffer is needed. Const types were supported before in that only
an asio::const_buffer could be constructed from the limited list of supported input types, but now the
metaprogramming asks STL like containers if their values are const (e.g. unordered_set), and if so
it will always generate an asio::const_buffer, which of course means that such STL like containers can
only be used for writing only and not reading.

* Fixed memory corruption in async_file_io_dispatcher destructor which only occurred when there were extant ops
during destruction.

* Use rename to random name before deletion pattern in rmfile() to work around Windows refusing new file
creation with the same name as a recently deleted file.


TODO BEFORE RELEASE

4. clang-reformat everything.

5. Run compilation time benchmarks on all platforms. Fix any regressions.
You might think this easy, but AFIO has about twenty different build
configurations now.

6. Run performance benchmarks on all platforms. Fix any regressions.



[/=================]
[heading Boost 1.57 AFIO v1.22 stable branch]
[/=================]

Fixed buffer underflow when decoding Win32 error codes to strings. Thanks to ariccio
for reporting this.

Relocated docs from ci.nedprod.com to http://boostgsoc13.github.io/boost.afio/

Updated the stale CI test dashboard copy in the DocBook edition.


[/=================]
[heading Boost 1.56 AFIO v1.21 10th Aug 2014]
[/=================]

Finished getting a ThreadSanitizer (tsan) + UndefinedBehaviorSanitizer (ubsan) pass
running per-commit on Travis CI (>= v1.2 was tsan clean, I just hadn't bothered getting
a CI to verify it to be so per commit).

Fixed bug where --lto wasn't turning on the optimiser for LTO output. Sorry.

Added a benchmark testing for latency under concurrency loads.

Added a new FAQ entry on AFIO execution latencies.

Reorganised source code structure to fit modular Boost. AFIO is now a Boost v1.56
module just like any other. Obviously this will break source code compatibility
with all preceding Boosts.

[/=================]
[heading Boost 1.55 AFIO v1.21 23rd Mar 2014]
[/=================]

Fixed a bug in the custom unit testing framework which was throwing away any exceptions
being thrown by the tests (thanks to Paul Kirth for finding this and reporting it).
Fixing this bug revealed that enumerate() with a glob on Windows has never worked
properly and the exception thrown by MSVC's checked iterators was hiding the problem,
so fixed that bug too.

Added async_io_dispatcher_base::post_op_filter() and async_io_dispatcher_base::post_readwrite_filter(),
including documentation examples and integrating filters into the unit testing.
post_readwrite_filter() ought to be particularly useful to those seeking deep ASIO
integration. Thanks to Bjorn Reese for the long discussions leading up to this
choice of improved ASIO support.

During updating the benchmarks below now I have regained access to my developer
workstation, discovered a severe performance regression in the v1.2 engine of
around 27% over the v1.1 engine. Steps taken:

1. The shared state in every async_io_op was a shared_ptr, now it is the
underlying shared_future. Eliminated copies of shared_ptr, now we always
use the shared_future in enqueued_task directly. This reduced regression to 18%.

2. Removed more code from inside the TSX locks. This reduced regression to 16%.

3. Removed the second TSX lock from complete_async_op(). This eliminated the
regression and actually added 2% to the v1.1 engine.

4. Removed the second TSX lock from chain_async_op(). This added a further 10%
over the v1.1 engine, so we are now 12% faster which is about right given
the v1.2 engine removed 15% of code.

Added nested TSX transaction support.

The CI shows that clang 3.1 now produces segfaulting binaries with this release,
so rather than debug clang, I simply dropped clang 3.1 support. AFIO now requires
clang 3.2 or better.

[/=================]
[heading Boost 1.55 AFIO v1.20 5th Feb 2014]
[/=================]

This is a major refactor of AFIO's core op dispatch engine to trim it down by
about 15%. Key breaking differences from the v1.1 series of AFIO are as follows:

* Replaced all use of packaged_task with enqueued_task, a custom implementation
which makes possible many performance improvements throughout the engine.
* thread_source::enqueue() now can take a preprepared enqueued_task.
* thread_source::enqueue() now always returns a shared_future instead of a future.
This has had knock on effects throughout AFIO, so many futures are now shared_future.
* Completion handler spec has changed from:
 
   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, shared_ptr<async_io_handle> h, exception_ptr *e)
 
  to:

   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, async_io_op preceding)
 
  This substantially improves performance, simplifies the implementation, and lets
  completion handlers more readily retrieve the error state of preceding operations
  and react appropriately.
* All restrictions on immediate completions have been removed. You can now do anything
in an immediate completion that you can do in a normal completion.
* async_io_op::h now always refers to a correct future i.e. the future is no longer
lazily allocated.
* Now that op futures are always correct, when_all(ops) has been drastically simplified
to an implementation which literally assembles the futures into a list and passes
them to boost::wait_for_all().
* Added when_any(ops).

[/=================]
[heading Boost 1.55 AFIO v1.11]
[/=================]

Added --fast-build to test Jamfile to preserve my sanity attempting to work with
AFIO on an Intel Atom 220 netbook.

Fixed failure to auto-const an async_data_op_req<boost::asio::mutable_buffer>
when used for writing. Thanks to Bjorn Reese for reporting this.

Replaced use of std::runtime_error with std::invalid_argument where that makes
sense. Thanks to Bjorn Reese for reporting this.

Replaced throwing of std::ios_base::failure with std::system_error. Thanks to
Bjorn Reese for suggesting and submitting a patch for this.

async_io_dispatcher_base::enumerate() did not take a metadata_flags, and it
was supposed to. Thanks to Bjorn Reese for reporting this.

Added a unit compilation test to ensure that implicit construction from a
single arg to the op convenience classes works as intended.

Significantly optimised build system and added in precompiled headers support.
Combined with --fast-build this provides an 8x build time improvement.

boost::afio::stat_t::st_type() is now a boost::filesystem::file_type instead
of replicating the POSIX file type codes. Thanks to Bjorn Reese for suggesting
this.

boost::afio::stat_t::st_mode() is now st_perms(). Also disabled unused fields in
stat_t on Windows. Thanks to Bjorn Reese for suggesting this.

[/=================]
[heading Boost 1.55 AFIO v1.1 1st Nov 2013]
[/=================]

Immediate completions no longer hold the opslock, which meant the opslock could be
changed from a recursive mutex to a spinlock. The new, more parallelised, behaviour
illuminated a number of new race conditions in when_all() which have been fixed.

Completely gutted dispatch engine and replaced with a new, almost entirely wait
free implementation based on throwing atomics at the problem. If it weren't for the spin lock around the
central ops hash table, AFIO would now be an entirely wait free design.

In order to do something about that spin lock, replaced all locking in AFIO (apart
from the directory file handle cache) with memory transactions instead. This
does CPUID at runtime and will use Intel's TSX-NI memory transaction implementation
if available, if not it falls back to a spin lock based emulation. On memory
transaction capable CPUs, AFIO is now almost entirely wait free, apart from when
it has to fetch memory from the kernel.

Made AFIO usable as headers only.

[/=================]
[heading Boost 1.55 AFIO v1.0 27th Sep 2013]
[/=================]

First release for end of Google Summer of Code 2013.

[endsect]




[section:FAQ Frequently Asked Questions]

[section:closure_performance What is the maximum throughput of AFIO's closure execution engine aka
how many IOPS can I push with AFIO?]

For v1.3 of the engine, maximum ops [*dispatch] throughput is approximately as follows, where the values for `call()` might be for
use as a closure engine whereas the values for `completion()` might be for max filing system IOPS[footnote The phrase
["might be] is important: a null closure benchmark will always have dispatch rate problems i.e. the closures being executed
take less time to execute than the time to dispatch them, so these figures are best read as maximum dispatch rate, not
maximum IOPS.]:

[table:throughput Maximum null closure dispatch rate on a 3.5Ghz Intel Core i7 3770K for AFIO v1.3[footnote Benchmarks compiled using `address-model=64 link=static --lto release -j 4`.]
[[Operating system][`call()` unchained][`call()` chained][`completion()` unchained][`completion()` chained][Raw ASIO]]
[[Microsoft Windows 8 x64 with Visual Studio 2013][[role alignright 1175030]][[role alignright 599038]][[role alignright 1634440]][[role alignright 743178]][[role alignright 2558220]]]
[[['Relative to ASIO]][[role alignright 48%]][[role alignright 25%]][[role alignright 69%]][[role alignright 27%]][[role alignright 100%]]]
[[['Relative to AFIO v1.21]][[role alignright +24%]][[role alignright +2%]][[role alignright +5%]][[role alignright -11%]][[]]]
[[['Relative to AFIO v1.1]][[role alignright +24%]][[role alignright +2%]][[role alignright +5%]][[role alignright -11%]][[]]]

[[Microsoft Windows 8 x64 with Visual Studio 2013][[role alignright 1133730]][[role alignright 605569]][[role alignright 1629110]][[role alignright 647738]][[role alignright 2376440]]]
[[['Relative to AFIO v1.1]][[role alignright +24%]][[role alignright +2%]][[role alignright +5%]][[role alignright -11%]][[]]]

[[][][][][][]]

[[Ubuntu 14.04 LTS Linux x64 with GCC 4.9.2][[role alignright 1002490]][[role alignright 1036150]][[role alignright 1467650]][[role alignright 1199460]][[role alignright 1686100[footnote Unfortunately the glibc mutex used by ASIO does not scale well on modern CPUs due to having too small a spin count, so this value is for four threads instead of eight (it was the best I could get through trial and error).]]]]
[[['Relative to ASIO]][[role alignright 80%]][[role alignright 64%]][[role alignright 92%]][[role alignright 75%]][[role alignright 100%]]]
[[['Relative to AFIO v1.21]][[role alignright +18%]][[role alignright +29%]][[role alignright +3%]][[role alignright +24%]][[role alignright 100%]]]
[[['Relative to AFIO v1.1]][[role alignright +18%]][[role alignright +29%]][[role alignright +3%]][[role alignright +24%]][[role alignright 100%]]]
[[][][][][][]]

[[Ubuntu 12.04 LTS Linux x64 with GCC 4.8][[role alignright 1291250]][[role alignright 1022970]][[role alignright 1479540]][[role alignright 1200450]][[role alignright 1611040[footnote Unfortunately the glibc mutex used by ASIO does not scale well on modern CPUs due to having too small a spin count, so this value is for four threads instead of eight (it was the best I could get through trial and error).]]]]
[[['Relative to AFIO v1.1]][[role alignright +18%]][[role alignright +29%]][[role alignright +3%]][[role alignright +24%]][[role alignright 100%]]]

]

clang 3.4 with    concurrent: 177682 157440 184407 138465 122453
clang 3.4 without concurrent: 180448 170979 187123 130066 

gcc 4.9 with    concurrent: 481860 504530 775678 878487
gcc 4.9 without concurrent:

We hope that ~600k min IOPS surely ought to be enough to max out any SATA III SSD __dash__ it should even max out any mid range
PCIe based SSD too, and if paired with a CPU with more cores, AFIO's almost wait free design ought to scale out fairly
well too such that it may be able to max out even top end PCIe SSDs, some of which can push 10m IOPS now.

[endsect] [/closure_performance]

[section:closure_latency What is the latency of AFIO's closure execution engine aka
how quickly can I push a single op with AFIO?]

For v1.3 of the engine on a quad core hyperthreading 3.5Ghz Intel Core i7 3770K, I found
the following latency graph which is in CPU cycles:

[role aligncenter [$afio_latencies.svg]]

[role aligncenter [$afio_latencies_1.2.svg]]

The solid lines represent average dispatch to execution and execution finish to completion
notification latencies and are cumulative on the left scale, whilst the dotted lines represent
minimum latencies and are cumulative on the right scale. As one can see, one can expect a
minimum of around 6,000 CPU cycles between dispatch and the operation beginning, and a minimum
of around 4,000 cycles between the operation completing and its op reference becoming signalled,
thus making total minimum latency per op around 10,000 CPU cycles, or around 2.5 microseconds.

Average latency is rather worse of course, but if you exceed CPU cores by just enough to stop
them clocking down you can expect a 300,000 CPU cycle total latency with a 95% confidence
interval of plus/minus 25,000 CPU cycles, which is 75 microseconds plus/minus 6.25 microseconds.
Obviously this makes AFIO unsuited for low latency work such as high frequency trading, but once
you get below 0.1 milliseconds it becomes very hard to disentangle SSD access times from the
latency of the filing and storage systems between you and the SSD anyway, so even if AFIO might
appear to on average double the 0.08 millisecond access time a SSD might have, you really must
bear in mind that a thread sleep and wake costs about 215,000 CPU cycles, and the obvious thing
which is happening in the benchmark is that AFIO is on average sleeping and waking 1.33 times
per operation dispatched and completed. Your use case will of course vary, and if you hit
the spin locks right I've personally seen 9,000 CPU cycle sustained average latencies.

[endsect] [/closure_latency]

[section:stuck_ops I'm seeing ["WARNING: `~async_file_dispatcher_base()` detects stuck `async_io_op` in total of X ops] during
process close. What does this mean?]

This means that you scheduled ops with a dispatcher which did not complete in a timely fashion before you tried to destroy
that dispatcher. This generally indicates a bug in your code, which can include:

# An op is still running in some thread pool and you didn't use __afio_when_all__ to wait for it to complete before trying
to destroy the dispatcher.
# An op's precondition never completed and therefore the op was never started.

Tracking down the cause of the latter in particular is very similar to tracking down race conditions i.e. it's hard, and we
as the authors of __boost_afio__ know just how hard! If you are on POSIX, recompiling AFIO with the macro
`BOOST_AFIO_OP_STACKBACKTRACEDEPTH` set to a reasonable depth like 8 will have AFIO take a stack backtrace for every op
allocated which it then will print during the stuck ops warnings. This can be helpful to identify which ops exactly are
stuck, and then you can figure out which preconditions of theirs are the cause.

Note that `BOOST_AFIO_OP_STACKBACKTRACEDEPTH` has two implementations, one based on glibc `backtrace()` and the other based
on libunwind. The latter is more portable, but requires explicit linking with libunwind, so we have defaulted to the former.

[endsect] [/stuck_ops]

[section:vector_use Why did you use `std::vector<>` as the ops batch container instead of a generic iterator range?]

# `std::vector<async_io_op>` is the closest thing to a variable length array in C++[footnote Ok, there is also
the oft-forgotten `std::valarray<>` too, but its use as a generic container isn't recommended.], at least until C++ 14 where we
will gain `std::dynarray<>` and dynamic array sizing.
# `std::vector<async_io_op>` is well understood, particularly its performance during by-value copies and during
`push_back()` which is by far the most common operation you do when preparing a batch.
# `std::vector<async_io_op>` is very amenable to splitting the batch across threads (not that AFIO currently does this).
# `std::vector<async_io_op>` is easily transportable through an ABI, whereas arbitrary container iterators would need type
erasing (i.e. slow). As AFIO was developed initially as not header-only, this made a lot of sense initially.

We are not opposed to the use of generic iterator ranges in an AFIO v2 if there is user demand for such a thing.

[endsect] [/vector_use]

[section:foreign_fd How do I configure my own file descriptor or HANDLE in AFIO?]

Sometimes you receive a file descriptor or HANDLE from a third party piece of code and you need to insert it as-in
into AFIO for use. The solution to this is very simple:

# Subclass __afio_handle__ with a custom implementation for your file descriptor type. In particular, you probably
will have custom close semantics (e.g. don't close, or invoke third party code to implement close).
# Instantiate your custom handle implementation, and pass it into `async_file_io_dispatcher_base::adopt()`. This
will immediately convert your custom handle type into an `async_io_op` suitable for supplying to `read()`, `write()`
etc.
# That's it, there is no more to it.

[endsect] [/foreign_fd]

[section:slow_compile Using AFIO really slows down my compile times. Can't you do something about that?]

You'll find a huge amount depends on your compiler. Here are some build time benchmarks for my developer workstation:

[table:build_time Single thread build times for AFIO v1.3 for various compilers and options on a 3.5Ghz Intel Core i7 3770K
[[Build flags][Microsoft Windows 8.1 x64 with Visual Studio 2013][Ubuntu 14.04 LTS Linux x64 with GCC 4.9 and gold linker][Ubuntu 14.04 LTS Linux x64 with clang 3.5 and gold linker]]
[  [`--link-test --fast-build debug`][][03m39s[footnote ASIO has a link error without `link=static`]][fails]]
[               [`--link-test debug`][][][]]
[         [`--link-test --lto debug`][[role red ]][][]]
[       [`--link-test pch=off debug`][][][]]
[[`--link-test --fast-build release`][][[footnote ASIO has a link error without `link=static`]][fails]]
[             [`--link-test release`][][][]]
[       [`--link-test --lto release`][][][]]
]

[table:build_time Single thread build times for AFIO v1.2 for various compilers and options on a 3.5Ghz Intel Core i7 3770K
[[Build flags][Microsoft Windows 8 x64 with Visual Studio 2013][Ubuntu 12.04 LTS Linux x64 with GCC 4.8 and gold linker][Ubuntu 12.04 LTS Linux x64 with clang 3.4 and gold linker]]
[  [`--link-test --fast-build debug`][01m43s][03m31s[footnote ASIO has a link error without `link=static`]][fails]]
[               [`--link-test debug`][02m40s][06m15s][04m24s]]
[         [`--link-test --lto debug`][[role red 09m36s]][06m23s][04m32s]]
[       [`--link-test pch=off debug`][06m12s][08m01s][04m26s]]
[[`--link-test --fast-build release`][02m01s][03m15s[footnote ASIO has a link error without `link=static`]][fails]]
[             [`--link-test release`][03m09s][06m28s][06m06s]]
[       [`--link-test --lto release`][10m24s][07m08s][06m14s]]
]

The benefit of `--fast-build` grows exponentially the less powerful the CPU: on my Intel Atom 220
netbook `--fast-build` gets total build times under five minutes which is impressive compared to
the twenty or so minutes without that flag.

`--fast-build` works by defining `BOOST_AFIO_HEADERS_ONLY` to 0 for each compiland, and
linking to the AFIO shared library instead. It also does the same for ASIO by defining `BOOST_ASIO_SEPARATE_COMPILATION`
and `BOOST_ASIO_DYN_LINK` for each compiland, and links to the ASIO
shared library instead (which is built via `boost/asio/impl/src.hpp`, see ASIO's docs).

[endsect] [/slow_compile]

[section:fatal_error_read Why do I get a fatal application exit with `FATAL EXCEPTION: Failed to read all buffers` when I read a file?]

This is actually a safety checkpoint for your code: in complex, multi-process concurrent reading and writing of
the same file, it is extremely difficult to coordinate changing file lengths with i/o in a way which
doesn't introduce race conditions OR unacceptably low performance. AFIO therefore doesn't even
try[footnote AFIO [*will] try to provide a synchronised, accurate file extent after fast portable
file locking support has been added, but until then no.]
and simply requires you the programmer to ALWAYS do i/o, whether reading or writing, within the extent
of a file. In other words, if you're going to read 100 bytes from offset 100 in a file, that file
better be at least 200 bytes long or it's going to fail with a fatal application exit.

This will probably seem harsh to anyone using AFIO for the first time, because the following
naive code will fatal exit the application if foo.txt is not 1024 bytes or longer:

[readallof_example_bad]

With synchronous i/o a read of 1024 bytes will read ['up to] 1024 bytes, returning the amount
actually read via some mechanism. With AFIO, either you
read [*all] 1024 bytes or you read nothing, in which case a normal exception is thrown with
whatever error the operating system returned. If a ['partial] read happens, then AFIO fatal
exits the application with the above message as it is probably a logic error in your code.

You may now wonder how to easily not exceed file extents during i/o: for writing, see
__afio_truncate__ to ensure a file's size before writing, or else open a file for append-only
in which case all writes atomically occur at the end of the file. For reading, the following code
is suggested:

[readallof_example_single]

If you're going to read many files from the same directory, it is far faster to open a
handle to the containing directory and using enumerate to fetch the metadata asynchronously
instead of using `direntry()` which is synchronous:

[readallof_example_many]

[endsect] [/fatal_error_read]

[section:async_metadata `async_io_handle::direntry()` and `async_io_handle::lstat()` are
both synchronous functions which block. How then can I get metadata about files and
directories asynchronously?]

This is easy, if not terribly obvious: call __afio_enumerate__ on the containing
directory of the file you want metadata for with a shell glob exactly matching the
file name and with the minimum metadata you are looking for. AFIO will then
asynchronously fetch that metadata for you, returning it in the future `directory_entry`
returned by `enumerate()`.

[endsect] [/async_metadata]

[section:deleting_open_files I thought on Windows it is impossible to delete a file which
is still open for use. How does AFIO achieve this?]

On POSIX you are allowed to call `unlink()` on a file which is open for use __dash__ indeed
this is a very convenient way of creating an anonymous private temporary file whose storage will be
deallocated on the last file handle close. Ordinarily Windows does not permit deletion or renaming of
files still in use, but if all open handles to that file were opened with `FILE_SHARE_DELETE` then
it does permit renames and deletions, BUT with an enormous caveat: creation of files with the
same name as a recently deleted file will be refused where ["recently] is a random period dependant
on the filing system's present mood. This turns out to be a major source on unexpected and random
errors for even AFIO's unit tests where the same file name tends to be reused across tests.

AFIO solves this by taking a dual step to deleting files. Firstly, it renames the file to a 128
bit cryptographically strong random name, and only then does it delete the file. As AFIO always
opens files with `FILE_SHARE_DELETE` permission enabled, with that flag Windows permits renaming
and deletion, and because the name was changed to a very random name before deletion, you don't
see those unexpected and random errors when creating files with the same name as a recently deleted
file as you do anywhere else on Windows. Handy eh?

[endsect] [/deleting_open_files]

[endsect] [/FAQ]
